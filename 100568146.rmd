---
output:
  pdf_document: default
  html_document: default
---
--------STATISTICAL TECHNIQUES---------


IMPORTING THE DATSET
```{r}
data=read.csv(file.choose())
head(data)

```

------T TEST------
Assumption:- the mean price between the two fuel types are eyal.
Null hypothesis H0:μ1-μ2=0 (the mean difference between the group is zero)
Alternate Hypothesis H1: μ1-μ2≠0 (there is a statistcal difference in the mean of two groups)
```{r}



library(ggplot2)        # plotting & data
library(dplyr)          # data manipulation
library(tidyr)          # data re-shaping
library(magrittr)       # pipe operator
library(gridExtra)      # provides side-by-side plotting
#Box pot for the mean of two groups
ggplot(data, aes(fueltype, price)) +
  geom_boxplot()
#distribution of the both groups
ggplot(data, aes(price)) +
  geom_histogram(fill = "white", color = "grey30") +
  facet_wrap(~ fueltype)
#distribution og the log of the dependent variable 
ggplot(data, aes(price)) +
  geom_histogram(fill = "white", color = "grey30") +
  facet_wrap(~ fueltype) +
  scale_x_log10()
 
```



```{r}
wilcox.test(price ~fueltype, data = data)
```

-------ANOVA------



```{r}
library(data.table) 
d3=data
setDT(d3)
#find mean points scored by team 
d3[ ,list(variance=var(price)), by=carbody]

hist(data$price)
```
--LEVENS TESTS FOR EQUALITY OF VARIANCE--
Null hypothesis H0: All five groups have equal variance
Alternative hypothesis H1:Atleast two of them differ
```{r}
d1=data
d1$CarName=as.factor(d1$CarName)
d1$fueltype=as.factor(d1$fueltype)
d1$aspiration=as.factor(d1$aspiration)
d1$doornumber=as.factor(d1$doornumber)
d1$carbody=as.factor(d1$carbody)
d1$drivewheel=as.factor(d1$drivewheel)
d1$enginelocation=as.factor(d1$enginelocation)
d1$enginetype=as.factor(d1$enginetype)
d1$cylindernumber=as.factor(d1$cylindernumber)
d1$fuelsystem=as.factor(d1$fuelsystem)
library(car)
# Using leveneTest()
result = leveneTest(price ~ carbody, d1)
print(result)


```
The p-value of the test is 6.299e-05 , which is less than our significance level of 0.05.
Thus, we reject the null hypothesis and conclude that the variance among the five groups is not equal.

--BARLETTS TESTS FOR EQUALITY OF VARIANCE--
```{r}
bartlett.test(price ~ carbody, d1)

```
p value is which is less than the alpha level of 0.05. This suggests that the samples do not all have equal variances




--CHECKING THE NORMALITY--
Conduct Shapiro-Wilk Test for normality 
```{r}
shapiro.test(data$price)

```

p-value is less than 0.05 hence its not a normal distribution

--KRUSKAL WALLIS TEST--
The distribution is not normaly distributed or doesnot have equal variances so hence
we are doing a Kruskal Wallis tests
The null hypothesis (H0): The median price across the five groups are equal.
The alternative hypothesis: (Ha): At least one of the median price is different from the others.
```{r}
kruskal.test(price ~ carbody, data=d1)

```
As the p-value is less than the significance level 0.05, we can conclude that there are significant 
differences between the treatment groups.

---DATA ANALYSIS---
Checking and dealing with multicollinearity
```{r}


library(mlbench)
library(ggplot2)
library(car)
library(caret)
library(corrplot)
# to find the variables which are correlated
mult=subset(d1,select=-c(price))
numeric= mult[ , c(2,10,11,12,13,14,17,19,20,21,22,23,24,25)]

#calculating correlation
correlation=cor(numeric)
print(correlation)
corrplot(correlation)
```
Some of the variables have high multicollinearity. And these variables should be treated before using a linear models or should use models that incorporate these multicollinear variables.
```{r}
require(caret)
highcorr=findCorrelation(correlation,cutof=0.8)
highcorrcol=colnames(numeric)[highcorr]
highcorrcol

```


The above variables are found to be the most correlated variables. These variables are deleted before doing a linear regression. The car_id is also removed since it is not a useful information in model building.
```{r}

#removing the correlated variables
d1_new=d1 %>% select(-1,-11,-14,-17,-24,-25)
#d1_new=d1[,-c('curbweight','carlength','highwaympg','enginesize','citympg','carwidth')]
str(d1_new)

```
--MULTIPLE LINEAR REGRESSION--
A multiple linear regression to predict the price of the data is built on the new data set which is the one we got after removing the multicollinear variables.



#model1

```{r}

l1=lm(price~., data=d1_new)
summary(l1)
```

The model l1 has a p value less than 0.05 and hence it is statistically significant and also the adjusted R square is 0.9566.
```{r}

par(mfrow = c(2, 2))
plot(l1)



```
Clearly the normality condition is satisfied from the Q-Q plot.
--DWTEST--
Test for auto correlated errors
H0: there is no autocorrelation between the errors

```{r}
#install.packages("lmtest")
library(lmtest)
dwtest(l1)



```
p value is greater than 0.05 hence we accept the null hypothesis that is the errors are independent.
--NCVTEST--
Testing constant error variance
```{r}
library(car)
ncvTest(l1)


```
p value> 0.05 hence we accept the null hypothesis that there is constant error variance.


Thus all the assumptions of regression is satisfied and the model is verified.



#Principal component regression 
```{r}
d1=data
d1$CarName=as.numeric(as.factor(d1$CarName))
d1$fueltype=as.numeric(as.factor(d1$fueltype))
d1$aspiration=as.numeric(as.factor(d1$aspiration))
d1$doornumber=as.numeric(as.factor(d1$doornumber))
d1$carbody=as.numeric(as.factor(d1$carbody))
d1$drivewheel=as.numeric(as.factor(d1$drivewheel))
d1$enginelocation=as.numeric(as.factor(d1$enginelocation))
d1$enginetype=as.numeric(as.factor(d1$enginetype))
d1$cylindernumber=as.numeric(as.factor(d1$cylindernumber))
d1$fuelsystem=as.numeric(as.factor(d1$fuelsystem))
```

```{r}
set.seed(123)
dat.d <- sample(1:nrow(d1),size=nrow(d1)*0.7,replace = FALSE) #random selection of 70% data.
train.data <- d1[dat.d,] # 70% training data
test.data <-d1[-dat.d,] # remaining 30% test data



```


```{r}
#install pls package (if not already installed)
#install.packages("pls")

#load pls package
library(pls)
```

```{r}
#make this example reproducible
set.seed(1)

#fit PCR model
model <- pcr(price~., data=d1, scale=TRUE, validation="CV")
```



```{r}
summary(model)

```
```{r}
#visualize cross-validation plots
validationplot(model)
validationplot(model, val.type="MSEP")
validationplot(model, val.type="R2")

```
The number of components is taken as eigth from the above graphs.
A model is built using these eigth components and its RMSE is calculated.
```{r}
model <- pcr(price~., data=d1, scale=TRUE, validation="CV")
pcr_pred <- predict(model, test.data, ncomp=8)
sqrt(mean((pcr_pred - test.data$price)^2))

```

--RIDGE REGRESSION--
```{r}
#install.packages("tidyverse")
library(tidyverse)
library(broom)
library(glmnet)
y=data$price
x=data%>%select(symboling,CarName,fueltype,aspiration,doornumber,carbody, drivewheel,
                enginelocation, wheelbase, carlength, carwidth, carheight, curbweight , enginetype,
                cylindernumber, enginesize, fuelsystem, boreratio,
                stroke, compressionratio, horsepower, peakrpm , citympg, highwaympg )%>%data.matrix()
lambdas=10^seq(3,-2,by=-.1)
l3=glmnet(x,y,alpha=0,lambda=lambdas)
cv_l3=cv.glmnet(x,y,alpha=0,lambda=lambdas)
plot(cv_l3)
opt=cv_l3$lambda.min
opt

```

The optimum lambda is found to be 501.1872.
```{r}
y_pred=predict(l3,s=opt,newx=x)
sst=sum(y^2)
sse=sum((y_pred-y)^2)
rsq=1-sse/sst
rsq
```
The r-square is 0.9725.


```{r}

#Lasso Regression
l4=glmnet(x,y,alpha=1,lambda=lambdas)
cv_l4=cv.glmnet(x,y,alpha=1,lambda=lambdas)
plot(cv_l4)
opt1=cv_l4$lambda.min
opt1


```
The optimum value is found and is used to predict the model.
```{r}
y_pred=predict(l3,s=opt1,newx=x)
sst=sum(y^2)
sse=sum((y_pred-y)^2)
rsq=1-sse/sst
rsq
```
The r-square is 0.9730.

---END---

